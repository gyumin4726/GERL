#!/usr/bin/env python3
"""
GERL ëª¨ë¸ì„ ìœ„í•œ ì´ë¶„ ê·¸ë˜í”„ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” MIND ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° ì‚¬ìš©ì-ë‰´ìŠ¤ ì´ë¶„ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
ë…¼ë¬¸ì— ë”°ë¼:
- "ë™ì¼í•œ ì‚¬ìš©ìê°€ ë³¸ ë‰´ìŠ¤ë“¤ì´ ì´ì›ƒ ë‰´ìŠ¤"
- "ë™ì¼í•œ ë‰´ìŠ¤ë¥¼ í´ë¦­í•œ ì‚¬ìš©ìë“¤ì´ ì´ì›ƒ ì‚¬ìš©ì"

ì‚¬ìš©ë²•:
    python build_graph.py --data_dir data/MIND_small
"""

import pandas as pd
import pickle
import os
import argparse
from collections import defaultdict
from tqdm import tqdm
import time


def load_mind_data(data_dir, split="train"):
    """MIND ë°ì´í„° ë¡œë“œ"""
    print(f"Loading {split} data from {data_dir}...")
    
    # ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
    news_path = f"{data_dir}/{split}/news.tsv"
    print(f"   Loading news data: {news_path}")
    news_df = pd.read_csv(
        news_path,
        sep='\t',
        header=None,
        usecols=[0, 1, 3]  # news_id, category, title
    )
    news_df.columns = ['news_id', 'category', 'title']
    
    # í–‰ë™ ë°ì´í„° ë¡œë“œ
    behaviors_path = f"{data_dir}/{split}/behaviors.tsv"
    print(f"   Loading behaviors data: {behaviors_path}")
    behaviors_df = pd.read_csv(
        behaviors_path,
        sep='\t',
        header=None
    )
    behaviors_df.columns = ['impression_id', 'user_id', 'time', 'clicked_news', 'impressions']
    
    print(f"Data loaded: {len(news_df)} news, {len(behaviors_df)} behaviors")
    return news_df, behaviors_df


def build_vocabulary(news_df, save_path):
    """ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•"""
    print("Building vocabulary...")
    
    word_count = defaultdict(int)
    
    # ëª¨ë“  ë‰´ìŠ¤ ì œëª©ì—ì„œ ë‹¨ì–´ ìˆ˜ì§‘
    for _, row in tqdm(news_df.iterrows(), total=len(news_df), desc="Processing news titles"):
        if pd.notna(row['title']):
            words = row['title'].lower().split()
            for word in words:
                word_count[word] += 1
    
    # ë¹ˆë„ ê¸°ì¤€ìœ¼ë¡œ ì–´íœ˜ êµ¬ì¶• (ìµœì†Œ ë¹ˆë„ 2 ì´ìƒ)
    vocab = {'<PAD>': 0, '<UNK>': 1}
    for word, count in word_count.items():
        if count >= 2:  # ìµœì†Œ ë¹ˆë„
            vocab[word] = len(vocab)
    
    # ì–´íœ˜ ì €ì¥
    with open(save_path, 'wb') as f:
        pickle.dump(vocab, f)
    
    print(f"Vocabulary built: {len(vocab)} words, saved to {save_path}")
    return vocab


def build_user_news_mappings(behaviors_df, news_df):
    """ì‚¬ìš©ì-ë‰´ìŠ¤ ë§¤í•‘ êµ¬ì¶•"""
    print("ğŸ”— Building user-news mappings...")
    
    # ë‰´ìŠ¤ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    news_dict = {}
    for _, row in news_df.iterrows():
        news_dict[row['news_id']] = {
            'category': row['category'],
            'title': row['title'] if pd.notna(row['title']) else "",
        }
    
    # ì‚¬ìš©ì í´ë¦­ íˆìŠ¤í† ë¦¬ êµ¬ì¶•
    user_clicked_news = defaultdict(list)
    news_to_users = defaultdict(set)
    user_to_news = defaultdict(set)
    
    print("   Processing user click histories...")
    for _, row in tqdm(behaviors_df.iterrows(), total=len(behaviors_df), desc="Processing behaviors"):
        user_id = row['user_id']
        
        if pd.notna(row['clicked_news']):
            clicked_list = row['clicked_news'].split()
            user_clicked_news[user_id].extend(clicked_list)
            
            # ì´ë¶„ ê·¸ë˜í”„ ë§¤í•‘ êµ¬ì¶•
            for news_id in clicked_list:
                if news_id in news_dict:
                    news_to_users[news_id].add(user_id)
                    user_to_news[user_id].add(news_id)
    
    print(f"Mappings built:")
    print(f"   Users: {len(user_to_news)}")
    print(f"   News with clicks: {len(news_to_users)}")
    print(f"   Total user-news interactions: {sum(len(news_set) for news_set in user_to_news.values())}")
    
    return news_dict, user_clicked_news, news_to_users, user_to_news


def build_neighbor_graphs(news_to_users, user_to_news):
    """ì´ì›ƒ ê·¸ë˜í”„ êµ¬ì¶• (ë…¼ë¬¸ì˜ í•µì‹¬ ë¡œì§)"""
    print("ğŸ•¸ï¸  Building neighbor graphs...")
    
    # ë…¼ë¬¸: "ë™ì¼í•œ ì‚¬ìš©ìê°€ ë³¸ ë‰´ìŠ¤ë¡œ ê°„ì£¼ë˜ì–´ ì´ì›ƒ ë‰´ìŠ¤ë¡œ ì •ì˜"
    print("   Building news neighbor graph...")
    news_neighbors = defaultdict(set)
    
    for user_id, news_set in tqdm(user_to_news.items(), desc="Processing users for news neighbors"):
        news_list = list(news_set)
        if len(news_list) > 1:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë‰´ìŠ¤ë¥¼ í´ë¦­í•œ ì‚¬ìš©ìë§Œ
            for i, news1 in enumerate(news_list):
                for j, news2 in enumerate(news_list):
                    if i != j:
                        news_neighbors[news1].add(news2)
    
    # ë…¼ë¬¸: "íŠ¹ì • ì‚¬ìš©ìëŠ” ë™ì¼í•œ í´ë¦­í•œ ë‰´ìŠ¤ë¥¼ ê³µìœ í•˜ì—¬ ì´ì›ƒ ì‚¬ìš©ìë¡œ ì •ì˜"
    print("   Building user neighbor graph...")
    user_neighbors = defaultdict(set)
    
    for news_id, user_set in tqdm(news_to_users.items(), desc="Processing news for user neighbors"):
        user_list = list(user_set)
        if len(user_list) > 1:  # ìµœì†Œ 2ëª… ì´ìƒì´ í´ë¦­í•œ ë‰´ìŠ¤ë§Œ
            for i, user1 in enumerate(user_list):
                for j, user2 in enumerate(user_list):
                    if i != j:
                        user_neighbors[user1].add(user2)
    
    print(f"Neighbor graphs built:")
    print(f"   News with neighbors: {len(news_neighbors)}")
    print(f"   Users with neighbors: {len(user_neighbors)}")
    
    # í†µê³„ ì¶œë ¥
    if news_neighbors:
        avg_news_neighbors = sum(len(neighbors) for neighbors in news_neighbors.values()) / len(news_neighbors)
        max_news_neighbors = max(len(neighbors) for neighbors in news_neighbors.values())
        print(f"   Avg news neighbors: {avg_news_neighbors:.1f}, Max: {max_news_neighbors}")
    
    if user_neighbors:
        avg_user_neighbors = sum(len(neighbors) for neighbors in user_neighbors.values()) / len(user_neighbors)
        max_user_neighbors = max(len(neighbors) for neighbors in user_neighbors.values())
        print(f"   Avg user neighbors: {avg_user_neighbors:.1f}, Max: {max_user_neighbors}")
    
    return news_neighbors, user_neighbors


def save_graph_data(graph_data, save_path):
    """ê·¸ë˜í”„ ë°ì´í„° ì €ì¥"""
    print(f"Saving graph data to {save_path}...")
    
    with open(save_path, 'wb') as f:
        pickle.dump(graph_data, f)
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = os.path.getsize(save_path) / (1024 * 1024)  # MB
    print(f"Graph saved: {file_size:.1f} MB")


def build_for_split(data_dir, split, vocab=None, force_rebuild=False):
    """íŠ¹ì • splitì— ëŒ€í•œ ê·¸ë˜í”„ êµ¬ì¶•"""
    print(f"\nProcessing {split} split...")
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    vocab_path = os.path.join(data_dir, "vocab.pkl")
    graph_path = os.path.join(data_dir, f"graph_{split}.pkl")
    
    # ê¸°ì¡´ íŒŒì¼ í™•ì¸
    if not force_rebuild and os.path.exists(graph_path):
        print(f"   Graph for {split} already exists: {graph_path}")
        return vocab
    
    start_time = time.time()
    
    # 1. ë°ì´í„° ë¡œë“œ
    news_df, behaviors_df = load_mind_data(data_dir, split)
    
    # 2. ì–´íœ˜ êµ¬ì¶• (trainì—ì„œë§Œ)
    if split == 'train':
        if force_rebuild or not os.path.exists(vocab_path):
            vocab = build_vocabulary(news_df, vocab_path)
        else:
            print(f"   Loading existing vocabulary from {vocab_path}")
            with open(vocab_path, 'rb') as f:
                vocab = pickle.load(f)
            print(f"   Vocabulary loaded: {len(vocab)} words")
    
    # 3. ì‚¬ìš©ì-ë‰´ìŠ¤ ë§¤í•‘ êµ¬ì¶•
    news_dict, user_clicked_news, news_to_users, user_to_news = build_user_news_mappings(
        behaviors_df, news_df
    )
    
    # 4. ì´ì›ƒ ê·¸ë˜í”„ êµ¬ì¶•
    news_neighbors, user_neighbors = build_neighbor_graphs(news_to_users, user_to_news)
    
    # 5. ê·¸ë˜í”„ ë°ì´í„° ì €ì¥
    graph_data = {
        'news_to_users': dict(news_to_users),
        'user_to_news': dict(user_to_news),
        'news_neighbors': dict(news_neighbors),
        'user_neighbors': dict(user_neighbors),
        'news_dict': news_dict,
        'user_clicked_news': dict(user_clicked_news)
    }
    
    save_graph_data(graph_data, graph_path)
    
    # ì™„ë£Œ ì‹œê°„ ì¶œë ¥
    elapsed_time = time.time() - start_time
    print(f"   {split.title()} completed in {elapsed_time:.1f}s")
    
    return vocab


def main():
    parser = argparse.ArgumentParser(description="Build bipartite graph for GERL model")
    parser.add_argument('--data_dir', default='data/MIND_small', help='MIND dataset directory')
    parser.add_argument('--split', default=None, choices=['train', 'dev'], 
                       help='Data split to process (default: both train and dev)')
    parser.add_argument('--force_rebuild', action='store_true', help='Force rebuild even if files exist')
    
    args = parser.parse_args()
    
    print(" GERL Graph Builder")
    print("=" * 50)
    print(f"Data directory: {args.data_dir}")
    
    # split ì¸ìê°€ ì—†ìœ¼ë©´ ë‘˜ ë‹¤ ì²˜ë¦¬
    if args.split is None:
        splits_to_process = ['train', 'dev']
        print("Processing: train and dev splits")
    else:
        splits_to_process = [args.split]
        print(f"Processing: {args.split} split only")
    
    print("=" * 50)
    
    # ê¸°ì¡´ íŒŒì¼ í™•ì¸ (ì „ì²´ì ìœ¼ë¡œ)
    vocab_path = os.path.join(args.data_dir, "vocab.pkl")
    all_exist = os.path.exists(vocab_path)
    for split in splits_to_process:
        graph_path = os.path.join(args.data_dir, f"graph_{split}.pkl")
        all_exist = all_exist and os.path.exists(graph_path)
    
    if not args.force_rebuild and all_exist:
        print(" All graph files already exist!")
        print("   Files found:")
        print(f"     {vocab_path}")
        for split in splits_to_process:
            graph_path = os.path.join(args.data_dir, f"graph_{split}.pkl")
            print(f"     {graph_path}")
        response = input("\nDo you want to rebuild? (y/N): ")
        if response.lower() != 'y':
            print(" Aborted by user")
            return
    
    total_start_time = time.time()
    vocab = None
    
    # trainì„ ë¨¼ì € ì²˜ë¦¬ (ì–´íœ˜ êµ¬ì¶•ì„ ìœ„í•´)
    if 'train' in splits_to_process:
        vocab = build_for_split(args.data_dir, 'train', vocab, args.force_rebuild)
    
    # dev ì²˜ë¦¬
    if 'dev' in splits_to_process:
        vocab = build_for_split(args.data_dir, 'dev', vocab, args.force_rebuild)
    
    # ì „ì²´ ì™„ë£Œ ì‹œê°„ ì¶œë ¥
    total_elapsed_time = time.time() - total_start_time
    print("\n" + "=" * 50)
    print("All graph building completed!")
    print(f" Total time: {total_elapsed_time:.1f} seconds ({total_elapsed_time/60:.1f} minutes)")
    print("=" * 50)
    
    print("\nSummary:")
    print(f"  Data directory: {args.data_dir}")
    if vocab:
        print(f"  Vocabulary: {len(vocab)} words")
    print(f"  Files created:")
    print(f"     {vocab_path}")
    for split in splits_to_process:
        graph_path = os.path.join(args.data_dir, f"graph_{split}.pkl")
        print(f"     {graph_path}")
    
    print("\n Now you can run training:")
    print("   python train.py --epochs 5 --batch_size 32")


if __name__ == "__main__":
    main() 